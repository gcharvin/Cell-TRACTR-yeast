lr: 0.0002
lr_backbone_names: ['backbone.0']
lr_backbone: 0.00002
lr_linear_proj_names: ['reference_points', 'sampling_offsets']
lr_linear_proj_mult: 0.1
lr_track: 0.0001
object_detection_only: false
overwrite_lrs: false
overwrite_lr_scheduler: false
batch_size: 2
weight_decay: 0.0001
epochs: 15
lr_drop: 12
flex_div: true
epoch_to_start_using_flexible_divisions: 5
use_prev_prev_frame: true
target_size: (256,32)
# gradient clipping max norm
clip_max_norm: 0.1
# Deformable DETR
deformable: true
with_box_refine: true
two_stage: true
# Model parameters
freeze_detr: false
load_mask_head_from_model: null
# Backbone
# Name of the convolutional backbone to use. ('resnet18, resnet34, resnet50', 'resnet101')
backbone: resnet18
# If true, we replace stride with dilation in the last convolutional block (DC5)
dilation: false
# Type of positional embedding to use on top of the image features. ('sine', 'learned')
position_embedding: sine
# Number of feature levels the encoder processes from the backbone
num_feature_levels: 4
# Transformer
# Number of encoding layers in the transformer
enc_layers: 4
# Number of decoding layers in the transformer
dec_layers: 4
# Intermediate size of the feedforward layers in the transformer blocks
dim_feedforward: 512
# Size of the embeddings (dimension of the transformer)
hidden_dim: 144 # needs to be divisible by 9 and 8 or 72
# Dropout applied in the transformer
dropout: 0.1
# Number of attention heads inside the transformer's attentions
nheads: 8
# Number of object queries
num_queries: 30
pre_norm: false
dec_n_points: 4
enc_n_points: 4
# DAB-DETR - denoising parameters
use_dab: true
dn_track: true
dn_track_add_object_queries: false
dn_track_l1: 0.2
dn_track_l2: 0.1
dn_object: false
dn_label: 0.3
dn_object_l1: 0.3
dn_object_l2: 0.15
dn_enc: false
dn_enc_l1: 0.3
dn_enc_l2: 0.15
enc_FN: 1
dn_track_group: true
tgt_noise: 0.00001
share_bbox_layers: false
# Embeddings to differentiate object vs track queries
refine_track_queries: true
refine_object_queries: true
refine_div_track_queries: false
use_div_ref_pts: false
init_enc_queries_embeddings: false
# Tracking
tracking: true
# In addition to detection also run tracking evaluation with default configuration from `cfgs/track.yaml`
tracking_eval: true
# Range of possible random previous frames
track_prev_prev_frame: true
track_backprop_prev_frame: False
# only for vanilla DETR
track_query_false_positive_eos_weight: true
track_attention: false
multi_frame_attention: false
multi_frame_encoding: false
multi_frame_attention_separate_encoder: false
merge_frame_features: false
overflow_boxes: true
# Segmentation
masks: true
mask_dim: 288
enc_masks: true
use_img_for_mask: true
# Matcher
# Class coefficient in the matching cost
set_cost_class: 4.0
# L1 box coefficient in the matching cost
set_cost_bbox: 5.0
# giou box coefficient in the matching cost
set_cost_giou: 2.0
set_cost_mask: 5.0
set_cost_dice: 5.0
match_masks: true
num_points: 10000
# Loss
# Disables auxiliary decoding losses (loss at each layer)
aux_loss: true
CoMOT: false
CoMOT_loss_ce: true
init_boxes_from_masks: false
decoder_use_mask_as_ref: false
iterative_masks: false
mult_enc_loss: false
return_intermediate_masks: false
dn_object_coef: 1.0
dn_track_coef: 1.0
pos_wei_loss_coef: 2.0
mask_loss_coef: 5.0
mask_weight_target_cell_coef: 10.0
dice_loss_coef: 5.0
loss_coef: 1.0 # This is just a formality
cls_loss_coef: 4.0 
bbox_loss_coef: 5.0
giou_loss_coef: 2.0
div_loss_coef: 2.0 
track_div_loss_coef: 2.0
object_queies_loss_coef: 1.0
no_data_aug: false
# Relative classification weight of the no-object class
eos_coef: 0.1
focal_loss: true
focal_alpha: 0.25
focal_gamma: 2
# Dataset
dataset: moma
# Number of plots
num_plots: 10
# Miscellaneous
# path where to save, empty for no saving
output_dir: /projectnb/dunlop/ooconnor/object_detection/cell-trackformer/results
data_dir: /projectnb/dunlop/ooconnor/object_detection/data
# device to use for training / testing
device: cuda
seed: 42
# resume from checkpoint
resume: ''
resume_shift_neuron: False
# resume optimization from checkpoint
resume_optim: true
# resume Visdom visualization
resume_vis: false
start_epoch: 1
eval_only: false
eval_train: false
num_workers: 0
val_interval: 5
debug: false
# epoch interval for model saving. if 0 only save last and best models
save_model_interval: false
# distributed training parameters
# number of distributed processes
world_size: 1
# url used to set up distributed training
dist_url: env://
cls_threshold: 0.5
iou_threshold: 0.5
display_all: True
hooks: False
avg_attn_weight_maps: True
